{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Customer Churn Prediction - Ensembles vs. Deep Learning\n",
    "\n",
    "**Objective:** Predict customer churn using ensemble methods (Random Forest, XGBoost) and a Deep Neural Network, and compare their performance on a tabular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Imbalance handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Note: To run this, first download the dataset from Kaggle and place 'WA_Fn-UseC_-Telco-Customer-Churn.csv' in the same directory.\n",
    "# https://www.kaggle.com/datasets/blastchar/telco-customer-churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Data (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nInfo:\")\n",
    "df.info()\n",
    "\n",
    "# Drop customerID as it's not a useful feature\n",
    "df = df.drop('customerID', axis=1)\n",
    "\n",
    "# Convert TotalCharges to numeric, coercing errors to NaN\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "# Handle missing values (e.g., new customers with 0 tenure might have no TotalCharges)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert target variable to binary\n",
    "df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "print(\"\\nChurn Distribution:\")\n",
    "print(df['Churn'].value_counts())\n",
    "sns.countplot(x='Churn', data=df)\n",
    "plt.title('Churn Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess Data and Handle Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessor on the training data and transform it\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Shape of training data before SMOTE: {X_train_processed.shape}\")\n",
    "print(f\"Class distribution before SMOTE: \\n{y_train.value_counts()}\")\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "print(f\"\\nShape of training data after SMOTE: {X_train_smote.shape}\")\n",
    "print(f\"Class distribution after SMOTE: \\n{y_train_smote.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dnn_model = build_dnn_model(X_train_smote.shape[1])\n",
    "dnn_model.summary()\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Note: It's good practice to create a validation set from the SMOTE'd training set for the DNN\n",
    "X_train_dnn, X_val_dnn, y_train_dnn, y_val_dnn = train_test_split(X_train_smote, y_train_smote, test_size=0.1, random_state=42)\n",
    "\n",
    "history = dnn_model.fit(\n",
    "    X_train_dnn,\n",
    "    y_train_dnn,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_dnn, y_val_dnn),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = {\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost': xgb_model,\n",
    "    'DNN': dnn_model\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models_to_evaluate.items():\n",
    "    print(f\"--- Evaluating {name} ---\")\n",
    "    if name == 'DNN':\n",
    "        y_pred_proba = model.predict(X_test_processed).ravel()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "        y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\\n\")\n",
    "    \n",
    "    results[name] = {'y_pred': y_pred, 'y_pred_proba': y_pred_proba, 'roc_auc': roc_auc}\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {result['roc_auc']:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"Conclusion: For this structured dataset, XGBoost provides a slight edge in performance over Random Forest and the DNN. \\n\",\n",
    "      \"This highlights that for many tabular data problems, gradient boosting models are an extremely strong, often state-of-the-art, choice.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}